{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm-lwZ3uUcz0"
   },
   "source": [
    "____\n",
    "### This tutorial provides each step to excute proposed procedures of Demposing models for the CPMG signal in N32.\n",
    "____\n",
    "Note1: To run this notebook, first click \"Open in playgound\" tab above. You may need \"Chrome\" browser and \"Gmail account\". Then in each cell, press 'Shift + Enter'\n",
    "\n",
    "Note2: It doesn't need any packages or programs and also doesn't store any files in your device."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2Iis-5iXejNC"
   },
   "source": [
    "from imports.models import *\n",
    "from imports.utils import *\n",
    "import adabound as Adabound\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "POOL_PROCESS = 23\n",
    "FILE_GEN_INDEX = 2\n",
    "pool = Pool(processes=POOL_PROCESS)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "import itertools"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0JMBh5OfGjy"
   },
   "source": [
    "___\n",
    "### At first, it'll generate simulation CPMG data for the denosing model.\n",
    "\n",
    "Practically, it needs more data. Just for this turorial, it'll generate a small number of samples. (training samples: 8192*4, validation sample: 8192)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "70hKTDP0ei33"
   },
   "source": [
    "time_resolution = 0.004\n",
    "time_data = np.arange(0, 60, time_resolution)\n",
    "\n",
    "MAGNETIC_FIELD = 403.553\n",
    "GYRO_MAGNETIC_RATIO = 1.07*1000       # Unit: Herts\n",
    "WL_VALUE = MAGNETIC_FIELD*GYRO_MAGNETIC_RATIO*2*np.pi\n",
    "N_PULSE_32 = 32\n",
    "N_PULSE_256 = 256\n",
    "\n",
    "SAVE_MLISTS = './'\n",
    "\n",
    "N_SAMPLES_TRAIN = 8192*4\n",
    "N_SAMPLES_VALID = 8192\n",
    "data_size = 1024\n",
    "GPU_INDEX = '0'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SujfbH8K2wG_"
   },
   "source": [
    "___\n",
    "This is for making gaussian slope in simulated data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XLRL1Gk9eivg"
   },
   "source": [
    "def gaussian_slope(time_data, time_index, px_mean_value_at_time, M_list=None):\n",
    "    m_value_at_time = (px_mean_value_at_time * 2) - 1\n",
    "    Gaussian_co = -time_data[time_index] / np.log(m_value_at_time)\n",
    "\n",
    "    slope = np.exp(-(time_data / Gaussian_co)**2)\n",
    "    if M_list != None:\n",
    "        M_list_slope = M_list * slope\n",
    "        px_list_slope = (1 + M_list_slope) / 2\n",
    "        return px_list_slope, slope\n",
    "    return slope\n",
    "\n",
    "SLOPE_INDEX = 11812\n",
    "MEAN_PX_VALUE = np.linspace(0.65, 0.94, 20)\n",
    "slope = {}\n",
    "for idx, mean_px_value in enumerate(MEAN_PX_VALUE):\n",
    "    slope[idx] = gaussian_slope(time_data[:24000], SLOPE_INDEX, mean_px_value, None)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUqPFsHZ2-g5"
   },
   "source": [
    "___\n",
    "Parameters to generate (A, B) lists.\n",
    "Maximum and minimum ranges in (A, B) values are determined w.r.t. distributions of nuclear spins in diamond. Roughly saying, the farther the distance from NV center is, the weaker the hyperfine interaction is, which means nuclear spins with smaller (A, B) values are distributed much more than nuclear spins with larger (A, B) values.\n",
    "\n",
    ": It can still be modified for better representation of accurate distributions of (A, B)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xCx3U0Greinp"
   },
   "source": [
    "#### Range of A & B (Hz)\n",
    "A_search_min = -80000\n",
    "A_search_max = 80000\n",
    "B_search_min = 2000\n",
    "B_search_max = 100000\n",
    "A_steps = 500\n",
    "B_steps = 500\n",
    "\n",
    "n_A_samples = (A_search_max - A_search_min) // A_steps\n",
    "n_B_samples = (B_search_max - B_search_min) // B_steps\n",
    "\n",
    "TOTAL_TEST_ARRAY = np.zeros((n_A_samples * n_B_samples, 2))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ayr4D4I2eibC"
   },
   "source": [
    "B_FIXED_VALUE = B_search_min\n",
    "for i in range(n_B_samples):\n",
    "    test_array = np.array([np.arange(A_search_min, A_search_max, A_steps), np.full((n_A_samples,), B_FIXED_VALUE)])\n",
    "    test_array = test_array.transpose()\n",
    "    test_array = np.round(test_array,2)\n",
    "    TOTAL_TEST_ARRAY[i*n_A_samples:(i+1)*n_A_samples] = test_array\n",
    "    B_FIXED_VALUE += B_steps"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ONKpzcYtgQ5X"
   },
   "source": [
    "A_candidate_max = 80000\n",
    "A_candidate_min = -80000\n",
    "B_candidate_max = 90000\n",
    "B_candidate_min = 2000\n",
    "\n",
    "A_small_max = 70000\n",
    "A_small_min = -70000\n",
    "B_small_max = 15000\n",
    "B_small_min = 2000\n",
    "\n",
    "candidate_boolen_index = ((TOTAL_TEST_ARRAY[:,1]>=B_candidate_min) & (TOTAL_TEST_ARRAY[:,1]<=B_candidate_max))   \\\n",
    "                        & ((TOTAL_TEST_ARRAY[:,0]>=A_candidate_min) & (TOTAL_TEST_ARRAY[:,0]<=A_candidate_max))\n",
    "AB_candidate_array = TOTAL_TEST_ARRAY[candidate_boolen_index]\n",
    "\n",
    "small_AB_boolen_index = ((TOTAL_TEST_ARRAY[:,1]>=B_small_min) & (TOTAL_TEST_ARRAY[:,1]<=B_small_max))   \\\n",
    "                         & ((TOTAL_TEST_ARRAY[:,0]>=A_small_min) & (TOTAL_TEST_ARRAY[:,0]<=A_small_max))\n",
    "small_AB_array = TOTAL_TEST_ARRAY[small_AB_boolen_index]\n",
    "\n",
    "AB_candidate_array *= (2 * np.pi)\n",
    "small_AB_array *= (2 * np.pi)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZxnbIfGWgRCJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "86ff20ad-e1ea-4eee-b08b-d500e49cdf1d"
   },
   "source": [
    "n_of_cases_train = 128\n",
    "n_of_samples = N_SAMPLES_TRAIN // n_of_cases_train\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(n_of_cases_train):\n",
    "\n",
    "    total_n_of_spin_lists = np.arange(24, 33)\n",
    "    n_of_spins = np.random.choice(total_n_of_spin_lists)\n",
    "    n_of_small_spins = n_of_spins // 2\n",
    "    n_of_spins, n_of_small_spins\n",
    "\n",
    "    globals()['ABlists_train_{}'.format(i)] = np.zeros((n_of_samples, n_of_spins, 2))\n",
    "\n",
    "    for idx in range(n_of_samples):\n",
    "\n",
    "        indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_spins-n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        globals()['ABlists_train_{}'.format(i)][idx, :n_of_spins-n_of_small_spins] = AB_candidate_array[indices_candi]\n",
    "\n",
    "        indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        globals()['ABlists_train_{}'.format(i)][idx, n_of_spins-n_of_small_spins:] = small_AB_array[indices_candi]\n",
    "\n",
    "print(\"ABlists for training dataset is generated: {} s\".format(round(time.time() - tic, 3)))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ABlists for training dataset is generated: 2.347 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "snjU-r6hgTzg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e52a5563-8fe6-4903-9b05-32071b4e648d"
   },
   "source": [
    "n_of_cases_valid = 32\n",
    "n_of_samples = N_SAMPLES_VALID // n_of_cases_valid\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(n_of_cases_valid):\n",
    "\n",
    "    total_n_of_spin_lists = np.arange(24, 36)\n",
    "    n_of_spins = np.random.choice(total_n_of_spin_lists)\n",
    "    n_of_small_spins = n_of_spins // 2\n",
    "    n_of_spins, n_of_small_spins\n",
    "\n",
    "    globals()['ABlists_valid_{}'.format(i)] = np.zeros((n_of_samples, n_of_spins, 2))\n",
    "\n",
    "    for idx in range(n_of_samples):\n",
    "\n",
    "        indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_spins-n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(AB_candidate_array), size=n_of_spins-n_of_small_spins)\n",
    "        globals()['ABlists_valid_{}'.format(i)][idx, :n_of_spins-n_of_small_spins] = AB_candidate_array[indices_candi]\n",
    "\n",
    "        indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        while len(set(indices_candi)) != (n_of_small_spins):\n",
    "            indices_candi = np.random.randint(len(small_AB_array), size=n_of_small_spins)\n",
    "        globals()['ABlists_valid_{}'.format(i)][idx, n_of_spins-n_of_small_spins:] = small_AB_array[indices_candi]\n",
    "\n",
    "print(\"ABlists for validation dataset is generated: {} s\".format(round(time.time() - tic, 3)))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ABlists for validation dataset is generated: 0.63 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f3JcqQF0rsN"
   },
   "source": [
    "___\n",
    "### Definition the generating function for denosing datasets.\n",
    ": the variable 'noise' should be set to be the experimental noise amplitude in user's experiment environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qG3V8qVngcIU"
   },
   "source": [
    "\n",
    "def Px_noise_data(time_table, wL_value, AB_list, n_pulse, rand_idx, data_size, y_train_pure=False):\n",
    "    noise = 0.043                                    # Maximum Height of Noise\n",
    "    rescale = np.random.random()/2 + 0.75\n",
    "    noise *= rescale\n",
    "\n",
    "    AB_list = np.array(AB_list)\n",
    "    A = AB_list[:,0].reshape(len(AB_list), 1)\n",
    "    B = AB_list[:,1].reshape(len(AB_list), 1)\n",
    "\n",
    "    w_tilda = pow(pow(A+wL_value, 2) + B*B, 1/2)\n",
    "    mz = (A + wL_value) / w_tilda\n",
    "    mx = B / w_tilda\n",
    "\n",
    "    alpha = w_tilda * time_table.reshape(1, len(time_table))\n",
    "    beta = wL_value * time_table.reshape(1, len(time_table))\n",
    "\n",
    "    phi = np.arccos(np.cos(alpha) * np.cos(beta) - mz * np.sin(alpha) * np.sin(beta))\n",
    "    K1 = (1 - np.cos(alpha)) * (1 - np.cos(beta))\n",
    "    K2 = 1 + np.cos(phi)\n",
    "    K = pow(mx,2) * (K1 / K2)\n",
    "    M_list_temp = 1 - K * pow(np.sin(n_pulse * phi/2), 2)\n",
    "    Y_train = np.prod(M_list_temp, axis=0)\n",
    "\n",
    "    slope_temp = np.zeros(data_size)\n",
    "    if np.random.uniform() > 0.4:\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp[:data_size//3] = slope[temp_idx][rand_idx:rand_idx+data_size//3]\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp[data_size//3:2*(data_size//3)] = slope[temp_idx][rand_idx+data_size//3:rand_idx+2*(data_size//3)]\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp[2*(data_size//3):data_size] = slope[temp_idx][rand_idx+2*(data_size//3):rand_idx+data_size]\n",
    "\n",
    "    else:\n",
    "        temp_idx = np.random.randint(len(slope))\n",
    "        slope_temp = slope[temp_idx][rand_idx:rand_idx+data_size]\n",
    "\n",
    "    if y_train_pure == False:\n",
    "        Y_train = (1 + slope_temp*Y_train) / 2\n",
    "        X_train = Y_train + noise*(np.random.random(Y_train.shape[0]) - np.random.random(Y_train.shape[0]))\n",
    "        return X_train, Y_train\n",
    "\n",
    "    else:\n",
    "        Y_train_pure = (1 + copy.deepcopy(Y_train)) / 2\n",
    "        Y_train = (1 + slope_temp*Y_train) / 2\n",
    "        X_train = Y_train + noise*(np.random.random(Y_train.shape[0]) - np.random.random(Y_train.shape[0]))\n",
    "        return X_train, Y_train, Y_train_pure"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zu40-3p0gcBp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "18973364-ad1e-4df2-a8a5-336859a60779"
   },
   "source": [
    "n_of_cases = 128\n",
    "\n",
    "X_train = np.zeros((N_SAMPLES_TRAIN, data_size))\n",
    "Y_train = np.zeros((N_SAMPLES_TRAIN, data_size))\n",
    "Y_train_pure = np.zeros((N_SAMPLES_TRAIN, data_size))\n",
    "\n",
    "for i in range(n_of_cases):\n",
    "    print(i, end=' ')\n",
    "    for j in range(len(ABlists_train_0)):\n",
    "        rand_idx = np.random.randint(11000)\n",
    "        time_data_temp = time_data[rand_idx:rand_idx+data_size]\n",
    "\n",
    "        X_train[i*len(ABlists_train_0)+j], \\\n",
    "        Y_train[i*len(ABlists_train_0)+j], \\\n",
    "        Y_train_pure[i*len(ABlists_train_0)+j] \\\n",
    "        = Px_noise_data(time_data_temp, WL_VALUE, globals()['ABlists_train_{}'.format(i)][j], N_PULSE_32, rand_idx, data_size, y_train_pure=True)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y2-2c5rAgboP"
   },
   "source": [
    "X_valid = np.zeros((N_SAMPLES_VALID, data_size))\n",
    "Y_valid = np.zeros((N_SAMPLES_VALID, data_size))\n",
    "Y_valid_pure = np.zeros((N_SAMPLES_VALID, data_size))\n",
    "\n",
    "for i in range(int(N_SAMPLES_VALID/ABlists_valid_0.shape[0])):\n",
    "    for j in range(len(ABlists_valid_0)):\n",
    "        rand_idx = np.random.randint(11000)\n",
    "        time_data_temp = time_data[rand_idx:rand_idx+data_size]\n",
    "\n",
    "        X_valid[i*len(ABlists_valid_0)+j], \\\n",
    "        Y_valid[i*len(ABlists_valid_0)+j], \\\n",
    "        Y_valid_pure[i*len(ABlists_valid_0)+j], \\\n",
    "        = Px_noise_data(time_data_temp, WL_VALUE, globals()['ABlists_valid_{}'.format(i)][j], N_PULSE_32, rand_idx, data_size, y_train_pure=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gv2hTMeuipsM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5427dc43-f18e-4b19-eff1-2f9364151a5d"
   },
   "source": [
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape, Y_train_pure.shape, Y_valid_pure.shape"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((32768, 1024),\n",
       " (32768, 1024),\n",
       " (8192, 1024),\n",
       " (8192, 1024),\n",
       " (32768, 1024),\n",
       " (8192, 1024))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1oBzVKM_n3pM"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import os, sys\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from imports.utils import *\n",
    "from imports.models import *\n",
    "from imports.adabound import AdaBound\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils import shuffle\n",
    "import itertools"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lo0oJsg0n5EE"
   },
   "source": [
    "X_train = np.expand_dims(X_train, axis=-2)\n",
    "Y_train = np.expand_dims(Y_train, axis=-2)\n",
    "X_valid = np.expand_dims(X_valid, axis=-2)\n",
    "Y_valid = np.expand_dims(Y_valid, axis=-2)\n",
    "\n",
    "X_train = torch.Tensor(X_train.reshape(X_train.shape[0], 2, -1)).cuda()\n",
    "Y_train = torch.Tensor(Y_train.reshape(X_train.shape[0], 2, -1)).cuda()\n",
    "X_valid = torch.Tensor(X_valid.reshape(X_valid.shape[0], 2, -1)).cuda()\n",
    "Y_valid = torch.Tensor(Y_valid.reshape(Y_valid.shape[0], 2, -1)).cuda()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zej9dCp5n8ll"
   },
   "source": [
    "____\n",
    "### Now, we'll build and train denoising model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A3RHqzp6kjSz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54687a40-d4b1-44de-acf0-da9881b3dc2f"
   },
   "source": [
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([32768, 2, 512]),\n",
       " torch.Size([32768, 2, 512]),\n",
       " torch.Size([8192, 2, 512]),\n",
       " torch.Size([8192, 2, 512]))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GwGwZEzKoN5E",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8c40bc4d-f4d1-4f17-e121-f7b6693c0da2"
   },
   "source": [
    "model = Denoise_Model().cuda()\n",
    "try:\n",
    "    pred = model(X_train[:128])\n",
    "    print(pred.shape)\n",
    "except:\n",
    "    raise NameError(\"The input shape should be revised\")\n",
    "total_parameter = sum(p.numel() for p in model.parameters())\n",
    "print('total_parameter: ', total_parameter / 1000000, 'M')\n",
    "print(model)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([128, 2, 512])\n",
      "total_parameter:  0.03437 M\n",
      "Denoise_Model(\n",
      "  (conv1d_1): Conv1d(2, 64, kernel_size=(4,), stride=(1,), padding=(2,))\n",
      "  (maxpooling): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1d_2): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(2,))\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convTrans1d_3): ConvTranspose1d(64, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (convTrans1d_4): ConvTranspose1d(64, 2, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cwj8EP6Yrksg"
   },
   "source": [
    "SAVE_DIR = './models/'\n",
    "os.mkdir(SAVE_DIR)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QYrGLY2q11FI",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "outputId": "61a9bea0-785e-49f8-a735-ab96ff245874"
   },
   "source": [
    "filename = 'denoising_model.pt'\n",
    "epochs = 30\n",
    "train_batch = X_train.shape[0]\n",
    "mini_batch = 64\n",
    "valid_mini_batch = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss().cuda()\n",
    "\n",
    "total_loss = []\n",
    "total_val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    avg_cost = 0\n",
    "    tic = time.time()\n",
    "\n",
    "    for i in range(train_batch // mini_batch):\n",
    "        train_indices = np.random.choice(train_batch, size=mini_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X_train[train_indices])\n",
    "        cost = criterion(hypothesis, Y_train[train_indices])\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost\n",
    "        print(round(((i+1)*mini_batch)/train_batch*100), '% in Epoch', end='\\r')\n",
    "    loss_temp = avg_cost / (train_batch // mini_batch)\n",
    "    total_loss.append(loss_temp.cpu().detach().item())\n",
    "    print(\"Epoch:\", '%4d' % (epoch + 1), ' | Loss =', '{:.5f}'.format(loss_temp), end=' | ')\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valid_indices = torch.randperm(X_valid.shape[0])[:valid_mini_batch]\n",
    "\n",
    "        prediction = model(X_valid[valid_indices])\n",
    "        val_loss = criterion(prediction, Y_valid[valid_indices])\n",
    "\n",
    "        print('Val_loss: {:.5f}'.format(val_loss.item()))\n",
    "        total_val_loss.append(val_loss.cpu().detach().item())\n",
    "    total_val_loss = np.array(total_val_loss)\n",
    "    if total_val_loss.min() >= total_val_loss[-1]:\n",
    "        torch.save(model.state_dict(), SAVE_DIR+filename)\n",
    "    else:\n",
    "        if np.min(total_val_loss[-3:-1]) < total_val_loss[-1]:\n",
    "            learning_rate *= 0.5\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    total_val_loss = list(total_val_loss)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch:    1  | Loss = 0.03145 | Val_loss: 0.00548\n",
      "Epoch:    2  | Loss = 0.00432 | Val_loss: 0.00332\n",
      "Epoch:    3  | Loss = 0.00300 | Val_loss: 0.00244\n",
      "Epoch:    4  | Loss = 0.00226 | Val_loss: 0.00193\n",
      "Epoch:    5  | Loss = 0.00183 | Val_loss: 0.00202\n",
      "Epoch:    6  | Loss = 0.00146 | Val_loss: 0.00128\n",
      "Epoch:    7  | Loss = 0.00115 | Val_loss: 0.00119\n",
      "Epoch:    8  | Loss = 0.00100 | Val_loss: 0.00115\n",
      "Epoch:    9  | Loss = 0.00090 | Val_loss: 0.00098\n",
      "Epoch:   10  | Loss = 0.00084 | Val_loss: 0.00107\n",
      "Epoch:   11  | Loss = 0.00077 | Val_loss: 0.00078\n",
      "Epoch:   12  | Loss = 0.00074 | Val_loss: 0.00076\n",
      "Epoch:   13  | Loss = 0.00072 | Val_loss: 0.00069\n",
      "Epoch:   14  | Loss = 0.00070 | Val_loss: 0.00101\n",
      "Epoch:   15  | Loss = 0.00068 | Val_loss: 0.00074\n",
      "Epoch:   16  | Loss = 0.00066 | Val_loss: 0.00066\n",
      "Epoch:   17  | Loss = 0.00066 | Val_loss: 0.00066\n",
      "Epoch:   18  | Loss = 0.00065 | Val_loss: 0.00064\n",
      "Epoch:   19  | Loss = 0.00065 | Val_loss: 0.00067\n",
      "Epoch:   20  | Loss = 0.00064 | Val_loss: 0.00064\n",
      "Epoch:   21  | Loss = 0.00064 | Val_loss: 0.00060\n",
      "Epoch:   22  | Loss = 0.00064 | Val_loss: 0.00058\n",
      "Epoch:   23  | Loss = 0.00064 | Val_loss: 0.00061\n",
      "Epoch:   24  | Loss = 0.00064 | Val_loss: 0.00060\n",
      "Epoch:   25  | Loss = 0.00064 | Val_loss: 0.00064\n",
      "Epoch:   26  | Loss = 0.00064 | Val_loss: 0.00062\n",
      "Epoch:   27  | Loss = 0.00064 | Val_loss: 0.00059\n",
      "Epoch:   28  | Loss = 0.00063 | Val_loss: 0.00065\n",
      "Epoch:   29  | Loss = 0.00063 | Val_loss: 0.00058\n",
      "Epoch:   30  | Loss = 0.00064 | Val_loss: 0.00063\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQYn1z0cxkjf"
   },
   "source": [
    "___\n",
    "The more the number of training datasets are, the better the performance is. When it reaches under around 0.0002~0.0003 in validation loss, then the denoising processing would be done enough."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMEPO5yvyrnh"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}
